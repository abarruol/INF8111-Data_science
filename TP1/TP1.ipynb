{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8111 - Data Mining\n",
    "\n",
    "## TP1 Fall 2021 - Duplicate Bug Report Detection\n",
    "\n",
    "##### Due date: 26/09 at 23:55\n",
    "##### The TP mark will be penalized by 10 points if  the notebook  takes more than 1 hour to be executed. \n",
    "\n",
    "##### Team Members:\n",
    "\n",
    "    - Name (student id) 1\n",
    "    - Name (student id) 2\n",
    "    - Name (student id) 3\n",
    "    \n",
    "    \n",
    "##### Deliverables:\n",
    "\n",
    "You must submit two separate files to Moodle:\n",
    "1. This notebook\n",
    "2. A [Json](https://en.wikipedia.org/wiki/JSON) containing scraped web-page content (bug_reports.json)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1 - Overview\n",
    "\n",
    "\n",
    "Due to complexities of software systems, software bugs are prevalent. Companies, especially large ones, usually use a Bug Tracking System (BTS), also called Issue Tracking System, to manage and track records of bugs. Besides developers and testers, many projects, mainly open source projects, allow users to report new bugs in their BTS.\n",
    "To do that, users have to fill a form with multiple fields. An important subset of\n",
    "these fields provides categorical data and only accepts values that range from a fixed list of\n",
    "options (e.g., component, version and product of the system). Another two important fields\n",
    "are the summary and the description. The users are free to write anything in both fields\n",
    "and the only constraint is that the summary has a maximum number of characters. The\n",
    "submission of a form creates a page, called bug report or issue report, which contains all\n",
    "the information about a bug.\n",
    "\n",
    "Due to the lack of communication and syncronization, users may not be aware that a specific bug was already submitted and report it again. Identifying duplicate bug reports is important task in the BTSs and it is the subject of this TP. Basically, our goal is to develop a system that will predicte wheather a pair of new bug report and submitted one is duplicate or not. This system will be used to manually identify duplicate reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the code below to install the packages needed for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                # If you want, you can use anaconda and install after nltk library\n",
    "# pip install --user numpy\n",
    "# pip install --user scikit-learn\n",
    "# pip install --user nltk\n",
    "# pip install --user tqdm\n",
    "\n",
    "\n",
    "#python\n",
    "#import nltk\n",
    "#nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Data\n",
    "\n",
    "Please download the zip file in the following url: https://www.dropbox.com/s/s53fqz29z8ch4ip/data.zip?dl=0\n",
    "\n",
    "In this zip file, there are: \n",
    "\n",
    "1. training.txt: This file contains pairs of bug reports that will be used to train our system.\n",
    "2. validation.txt: This file contains  pairs of bug reports that will be used to evaluate our system.\n",
    "2. bug_reports: It is a folder that contains the bug report html source. Each html file name follows the pattern **bug_report_id.html**.\n",
    "\n",
    "\n",
    "Figure below depicts an bug report page example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "display.Image(\"https://irving-muller.github.io/images/bug-report-eclipse.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A : bug report id\n",
    "- B : creation date\n",
    "- C : summary\n",
    "- D : product\n",
    "- E : component\n",
    "- F : the report id which the bug report is duplicate\n",
    "- G : description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script loads the test dataset and define some global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define the folder path that contain the data\n",
    "# FOLDER_PATH = \"Define folder path that contain threads folder,training_eclipse.txt, and validation.txt\"\n",
    "FOLDER_PATH = \"data/\"\n",
    "PAGE_FOLDER = os.path.join(FOLDER_PATH, 'bug_reports')\n",
    "\n",
    "\n",
    "# Load the evaluation dataset\n",
    "import json\n",
    "\n",
    "\n",
    "training_file = open(os.path.join(FOLDER_PATH, \"training.txt\"))\n",
    "validation_file = open(os.path.join(FOLDER_PATH, \"validation.txt\"))\n",
    "word_vec_path = os.path.join(FOLDER_PATH, \"glove.42B.300d_clear.txt\")\n",
    "\n",
    "def read_dataset(f):\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if len(line) == 0:\n",
    "            continue\n",
    "        \n",
    "        rep1, rep2, label = line.split(',')\n",
    "\n",
    "        rep1 = int(rep1)\n",
    "        rep2 = int(rep2)\n",
    "        label = 1.0 if int(label) > 0 else 0.0 \n",
    "        \n",
    "        yield (rep1, rep2, label)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "training_pairs = list(read_dataset(training_file))\n",
    "validation_pairs = list(read_dataset(validation_file))\n",
    "\n",
    "training_reports_set = set()\n",
    "\n",
    "\n",
    "for r1, r2, _ in training_pairs:\n",
    "    training_reports_set.add(r1)\n",
    "    training_reports_set.add(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Web scraping\n",
    "\n",
    "Web scraping consists in extracting relevant data from pages and prepare it for computational analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Question 1 (4 points)\n",
    "\n",
    "Implement extract_data_from_page function. This function extracts the following information from the html: bug report id, creation date, title, product, component, the report id which it is duplicate and description.\n",
    "\n",
    "*extract_data_from_page* function returns a dictionary with the following structure:\n",
    "\n",
    "\n",
    " ```python\n",
    "{\"report_id\": int, \n",
    "  \"dup_id\": int or None (the report id which it is duplicate), \n",
    "  \"component\": string, \n",
    "  \"product\": string, \n",
    "  \"summary\": string, \n",
    "  \"description\": string, \n",
    "  \"creation_date\": int}\n",
    " ```\n",
    "For instance, when extract_data_from_page receives \"bug_report/18431.html\", it returns:\n",
    " \n",
    "```python\n",
    "{'report_id': 18431,\n",
    " 'dup_id': 27227,\n",
    " 'component': 'GEF-Legacy Draw2d',\n",
    " 'product': 'GEF',\n",
    " 'summary': 'Polylines ignored by FLowLayout',\n",
    " 'description': 'I tried a poor sample but it\\'s not working the way I thought it would be. Look \\nat the following source. I have a figure with FlowLayout and add 2 polylines to \\nit but the polylines aren\\'t arranged in FlowLayout. Both polylines are pushed \\ninto the first flow element. Any ideas why?\\n\\n\\n\\npublic static void main(String args[])\\n{\\n Shell shell = new Shell();\\n shell.open();\\n shell.setText(\"draw2d Figures\");\\n\\n LightweightSystem lws = new LightweightSystem(shell);\\n\\n IFigure panel = new Figure();\\n panel.setLayoutManager( new FlowLayout(FlowLayout.HORIZONTAL) );\\n\\n lws.setContents(panel);\\n\\n Polyline polyline = new Polyline();\\n polyline.setStart(new Point( 5, 5));\\n polyline.addPoint(new Point( 5, 45));\\n polyline.addPoint(new Point( 45, 45));\\n polyline.addPoint(new Point( 45, 5));\\n panel.add(polyline);\\n\\n Polyline polyline2 = new Polyline();\\n polyline2.setStart(new Point( 5, 5));\\n polyline2.addPoint(new Point( 45, 45));\\n panel.add(polyline2);\\n\\n Display display = Display.getDefault();\\n while( !shell.isDisposed() )\\n {\\n  if( !display.readAndDispatch() )\\n   display.sleep();\\n }\\n\\n}',\n",
    " 'creation_date': '2002-05-31 09:17 EDT'}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creation date have to be represented as \"year-mont-day hour:minute timezone\". If bug report is not duplicate, dup_id have to be None.**\n",
    "\n",
    "*HINT: lxml parse is faster than html.parser*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_page(pagepath):\n",
    "    \"\"\"\n",
    "    Scrap bug report content from bug report html.\n",
    "    \n",
    "    :param pagepath: the path of html file.\n",
    "    :return: \n",
    "        {\n",
    "        \"report_id\": int,\n",
    "        \"dup_id\": int or None (the report id which it is duplicate), \n",
    "        \"component\": string, \n",
    "        \"product\": string, \n",
    "        \"summary\": string, \n",
    "        \"description\": string, \n",
    "        \"creation_date\": string\n",
    "        }\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Extract text from HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool, TimeoutError\n",
    "from time import time\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "# Index each report by its id\n",
    "index_path = os.path.join(FOLDER_PATH, 'bug_reports.json')\n",
    "\n",
    "if os.path.isfile(index_path):\n",
    "    # Load threads that webpage content were already extracted.\n",
    "    report_index = json.load(open(index_path))\n",
    "else:\n",
    "    # Extract webpage content\n",
    "\n",
    "    # This can be slow (around 15 minutes). Test your code with a small sample. lxml parse is faster than html.parser\n",
    "    files = [os.path.join(PAGE_FOLDER, filename) for filename in os.listdir(PAGE_FOLDER)]\n",
    "    reports = [extract_data_from_page(f) for f in tqdm.tqdm(files)]\n",
    "    report_index = dict(((report['report_id'], report) for report in reports ))\n",
    "\n",
    "    \n",
    "    json.dump(report_index, open(index_path,'w'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Data Preprocessing\n",
    "\n",
    "Preprocessing is a crucial task in data mining. This task clean and transform the raw data in a format that can better suit data analysis and machine learning techniques. In natural language processing (NLP), *tokenization* and *stemming* are two well known preprocessing steps. Besides these two steps, we will implement an additional step that is designed exclusively for the twitter domain.\n",
    "\n",
    "## 5.1 - Tokenization\n",
    "\n",
    "In this preprocessing step, a *tokenizer* is responsible for breaking a text in a sequence of tokens (words, symbols, and punctuations). For instance, the sentence *\"It's the student's notebook.\"* can be split into the following list of tokens: ['It', \"'s\", 'the', 'student', \"'s\", 'notebook', '.'].\n",
    "\n",
    "\n",
    "### 5.1.1 - Question 2 (1 point) \n",
    "\n",
    "**Implement** ```tokenize_space_punk``` **that replaces the punctuation to space and then tokenizes the tokens that are separated by whitespace (space, tab, newline). You have to lowercase the tokens.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_space_punk(text):\n",
    "    \"\"\"\n",
    "    This tokenizer replaces punctuation to spaces and then tokenizes the tokens that are separated by whitespace (space, tab, newline).\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 - Stop words removal\n",
    "\n",
    "### 5.2.1 - Question 3 (0.5 points)\n",
    "\n",
    "There are a set of tokens, call stop words, that are not signficant to the similarity comparison since they appear in most of bug report pages. Thus, removing them decreases the vector dimensionality and turns the similarity calculation computationally cheaper. Describe the tokens that can be removed without affecting the similarity comparison. Moreover, implement the function filter_tokens that removes these words from a list of tokens.\n",
    "\n",
    "**You can use a predefined set of words from a library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tokens(tokens):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 - Stemming\n",
    "\n",
    "The process to convert tokens with the same stem (word reduction that keeps word prefixes) to a standard form is called *stemming*. For instance, the word \"fishing\", \"fished\" , \"fish\" and \"fisher\" are reduced to the stem \"fish\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "word1 = ['I', 'tried', 'different', 'fishes']\n",
    "\n",
    "print([ stemmer.stem(w) for w in word1])\n",
    "\n",
    "word2 = ['I', 'will', 'tries', 'only', 'one', 'fishing']\n",
    "print([ stemmer.stem(w) for w in word2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 - Question 4 (0.5 point) \n",
    "\n",
    "*Expliquez comment et pourquoi le stemming est utile à notre système.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Data Representation\n",
    "\n",
    "\n",
    "# 6.1 - Bag of Words (BoW)\n",
    "\n",
    "\n",
    "Many algorithms only accept inputs that have the same size. However, there are some data types whose sizes are not fixed, for instance, a text can have an unlimited number of words. Imagine that we retrieve two sentences: ”Board games are much better than video games” and ”Monopoly is an awesome game!”. These sentences are respectively named as Sentence 1 and 2. Table below depicts how we could represent both sentences using a fixed representation.\n",
    "\n",
    "|            | an | are | ! | monopoly | awesome | better | games | than | video | much | board | is | game |\n",
    "|------------|----|-----|---|----------|---------|--------|-------|------|-------|------|-------|----|------|\n",
    "| Sentence 1 | 0  | 1   | 0 | 0        | 0       | 1      | 2     | 1    | 1     | 1    | 1     | 0  | 0    |\n",
    "| Sentence 2 | 1  | 0   | 0 | 1        | 1       | 0      | 0     | 0    | 0     | 0    | 0     | 1  | 1    |\n",
    "\n",
    "Each column of this table 2.1 represents one of 13 vocabulary words, whereas the rows contains the word\n",
    "frequencies in each sentence. For instance, the cell in row 1 and column 7 has the value 2\n",
    "because the word games occurs twice in Sentence 1. Since the rows have always 13 values, we\n",
    "could use those vectors to represent the Sentences 1 and 2. The table above illustrates a technique called bag-of-words. Bag-of-words represents a document as a vector whose dimensions are equal to the number of times that vocabulary words appeared in the document. Thus, each token will be related to a dimension, i.e., an integer.\n",
    "\n",
    "*For simplicity, we consider token and word as interchangeable.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 - TF-IDF\n",
    "\n",
    "\n",
    "Using raw frequency in the bag-of-words can be problematic. The word frequency distribution\n",
    "is skewed - only a few words have high frequencies in a document ([Zipf law](https://en.wikipedia.org/wiki/Zipf%27s_law)). Consequently, the\n",
    "weight of these words will be much bigger than the other ones which can give them more\n",
    "impact on some tasks, like similarity comparison. Besides, words (including\n",
    "those with high frequency) that appear in most of the documents cannot help discriminating the documents. For instance, the word *of* appears in a significant\n",
    "portion of reports. Thus, having the word *of* does not make\n",
    "reports more or less similar. However, the word *terrible* is rarer and reports that\n",
    "have this word are more likely to be negative. TF-IDF is a technique\n",
    "to overcome the word frequency disadvantages.\n",
    "\n",
    "\n",
    "TF-IDF weights the vector using inverse document frequency (IDF) and word frequency, called term frequency (TF).\n",
    "TF is the local information about how important is a word to a specific document.  IDF measures the discrimination level of the words in a dataset.  Common words in a domain are not helpful to discriminate documents since most of them contain these terms. So, to reduce their relevance in the documents, these words should have low weights in the vectors . \n",
    "The following equation calculates the word IDF:\n",
    "\\begin{equation}\n",
    "\t\\operatorname{IDF}(t) = \\ln\\left( \\frac{N+1}{\\operatorname{df}(t)+1} \\right) + 1,\n",
    "\\end{equation}\n",
    "where $t$ is a token, $N$ is the number of documents in the dataset, and $\\operatorname{df}(\\cdot)$ is the number of documents that contain a word $i$.\n",
    "The new weight of a word $t$ in a text is defined as:\n",
    "\\begin{equation}\n",
    "\tw(t) = \\operatorname{tf}(t) \\times \\operatorname{IDF}(t),\n",
    "\\end{equation}\n",
    "where $\\operatorname{tf}(\\cdot)$ is the term frequency of word $i$ , i.e, number of times that a word appears in the document. *We call TF-IDF representation when the weights of the BoW represention are computed by means of TF-IDF.*\n",
    "\n",
    "### 6.2.1 - Question 5 (3.5 points)\n",
    "\n",
    "\n",
    "*In this TP, we represent each text as list of tuples in which the tuple are composed of the token and its weight computed by TF-IDF. To improve efficiency, we sort the tuples by the tokens. Thus, for instance, Sentence 1 of previous table is represented as: ```[ (\"are\", 1.4054), (\"better\", 1.4054), (\"board\", 1.4054), (\"games\", 2.8108), (\"much\", 1.4054), (\"than\", 1.4054), (\"video\", 1.4054)]```.*\n",
    "\n",
    "\n",
    "\n",
    "**Now, you have to implement TF-IDF. Method** ```fit``` **computes IDF based on the textual data in X and method** ```transform``` **transforms a text to a TF-IDF representation.\n",
    "\n",
    "*Notes*:\n",
    "\n",
    "- Be careful that ```transform```can receive tokens that were not in X. In this case, consider that $\\operatorname{df}(\\cdot)=0$.\n",
    "- For this question, except numpy, you cannot use any external python library (e.g., scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF:\n",
    "    \n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Learn IDF values based on the textual data in X\n",
    "        \n",
    "        X:  list of lists of tokens. For instance, X=[['video','awesome', 'The'], ['house', 'The']] \n",
    "\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        pass\n",
    "            \n",
    "              \n",
    "\n",
    "    def transform(self, tokens):\n",
    "        \"\"\"\n",
    "        Transform a list of tokens to TF-IDF representation.\n",
    "        \n",
    "        tokens: list of tokens. For instance, tokens=['video','awesome', 'The', 'The'] \n",
    "        \n",
    "        :return: list of tuples. Return the TF-IDF representation of the texts.  \n",
    "                For instance, [('video', 1.4054), ('awesome', 1.4054), ('The', 2.0)]\n",
    "        \"\"\"\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 - Word embedding\n",
    "\n",
    "Recently, a new type of representation, called word embeddings, word vectors, or distributed word representation, has been shown very useful for NLP. In word embeddings, words are represented as real, low dimensional and dense vectors. Those vectors describe word positions in a new feature space that retain syntactic and semantic information. In contrast to other representations, word embeddings mitigate the curse of dimensionality and improve\n",
    "the model capability to handle unknown and rare words in the training. Furthermore,\n",
    "using distributed word representations, it is possible to perform arithmetical operations and\n",
    "calculate the distance between words. \n",
    " \n",
    "\n",
    "### 6.3.1 - Question 6 (3 points)\n",
    "\n",
    "In this TP, we will use word embeddings to generate a dense representation of text, called text embedding.\n",
    "In this context, text could contain a sentence or multiple paragraphs.\n",
    "The text embedding is computed as average of the vectors of the words:\n",
    "\\begin{equation}\n",
    "\te_s = \\frac{1}{|s|} \\sum_{t \\in s} w_t,\n",
    "\\end{equation}\n",
    "where $|s|$ is the length of text $s$, $w_t$ is the word embedding of token $t$ in $s$, and $e_s$ is the embedding of $s$.\n",
    "\n",
    "You will use the pre-trained word embeddings from *glove.42B.300d_clear.txt* in the folder *dataset*.\n",
    "In each line of this text file, there are the tokens and theirs' vector values. Values and tokens are separated by spaces. In this file, the word embedding dimension is 300.\n",
    "\n",
    "****Now, you have to implement the method** ```generate_embedding``` **that generates the text embedding. The attribute token2vec is a dictionary that links a word to its vector.****\n",
    "\n",
    "*Notes:*\n",
    "- Disconsider words that does not exist in glove.42B.300d_clear.txt\n",
    "- For this question, except for numpy, you cannot use any external python library (e.g., scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding:\n",
    "    \n",
    "    def __init__(self):\n",
    "        word_vec_file = open(word_vec_path)\n",
    "        self.token2vec = {}\n",
    "        \n",
    "        #TODO: load the file content\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def generate_embedding(self, tokens):\n",
    "        \"\"\"\n",
    "        Generate the text embedding as the average of the word embedding in tokens.\n",
    "        \n",
    "        Disconsider words that does not exist in glove.42B.300d_clear.txt.\n",
    "\n",
    "        tokens: list of tokens. E.g., [\"are\", \"better\", \"board\", \"much\"]\n",
    "\n",
    "        :return:  text embedding (numpy or list of real numbers)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Pipeline\n",
    "\n",
    "The pipeline is a sequence of preprocessing steps that transform the raw data to a format that is suitable for your problem. \n",
    "\n",
    "\n",
    "\n",
    "## 7.1 - Question 7 (2 points) \n",
    "\n",
    "**Now, you have to implement the pipeline following the instructions in the cells below.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After the pipeline execution, each report in report_index should contain four keys:\n",
    "    - summary_tfidf: TF-IDF representation of the summary\n",
    "    - desc_tfidf: TF-IDF representation of the descripton\n",
    "    - summary_vec: the text embedding of the summary\n",
    "    - desc_vec: the text embedding of the description\n",
    "\n",
    "For example, report 18431 in report_index (report_index[18431]) is:\n",
    "\n",
    "{'report_id': 18431,\n",
    " 'dup_id': 27227,\n",
    " 'component': 'GEF-Legacy Draw2d',\n",
    " 'product': 'GEF',\n",
    " 'summary': 'Polylines ignored by FLowLayout',\n",
    " 'description': 'I tried a poor sample but it\\'s not working the way I thought it would be. Look \\nat the following source. I have a figure with FlowLayout and add 2 polylines to \\nit but the polylines aren\\'t arranged in FlowLayout. Both polylines are pushed \\ninto the first flow element. Any ideas why?\\n\\n\\n\\npublic static void main(String args[])\\n{\\n Shell shell = new Shell();\\n shell.open();\\n shell.setText(\"draw2d Figures\");\\n\\n LightweightSystem lws = new LightweightSystem(shell);\\n\\n IFigure panel = new Figure();\\n panel.setLayoutManager( new FlowLayout(FlowLayout.HORIZONTAL) );\\n\\n lws.setContents(panel);\\n\\n Polyline polyline = new Polyline();\\n polyline.setStart(new Point( 5, 5));\\n polyline.addPoint(new Point( 5, 45));\\n polyline.addPoint(new Point( 45, 45));\\n polyline.addPoint(new Point( 45, 5));\\n panel.add(polyline);\\n\\n Polyline polyline2 = new Polyline();\\n polyline2.setStart(new Point( 5, 5));\\n polyline2.addPoint(new Point( 45, 45));\\n panel.add(polyline2);\\n\\n Display display = Display.getDefault();\\n while( !shell.isDisposed() )\\n {\\n  if( !display.readAndDispatch() )\\n   display.sleep();\\n }\\n\\n}',\n",
    " 'creation_date': '2002-05-31 09:17 EDT',\n",
    " 'summary_vec': array([x.xxxxx, x.xxxxx, ... ]),\n",
    " 'desc_vec':  array([x.xxxxx, x.xxxxx, ... ]),\n",
    " 'summary_tfidf': [['flowlayout', x], ['ignor', x], ['polylin', x]],\n",
    " 'desc_tfidf': [['2', x], ['45', x], ['5', x], ['add', x], ['addpoint', x], ... ]}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Step 1. Tokenize and remove the stop words in the summary and description of each report.\n",
    "\"\"\"\n",
    "#TODO: implement step 1\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Step 2. Generate the embedding of the summary and description for each report. \n",
    "Use the texts preprocessed by step 1 to generate the embeddings.\n",
    "\"\"\"\n",
    "\n",
    "#TODO: implement step 2\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 3. Apply stemming to the tokens generated from Step 1. \n",
    "\"\"\"\n",
    "#TODO: implement step 3\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 4. Learn the IDF using the summary and description of all the repors in the training set.\n",
    "You should concatenate the content of these two fields and a report will be considered a document in order to computer the document frequency.\n",
    "The input of this step should be output of Step 3 (stemmed tokens).\n",
    "\"\"\"\n",
    "# training_reports_set contains all reports in the training set.\n",
    "training_reports_set\n",
    "    \n",
    "# TODO: implement step 4\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 5. Generate the TF-IDF representation of the summary and description for each report.\n",
    "The input of this step should be output of Step 3 (stemmed tokens).\n",
    "\"\"\"\n",
    "# TODO: implement step 5\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Cosine Similarity\n",
    "\n",
    "In NLP, the cosine similarity is a popular similarity function used to compare the vectors\n",
    "of documents. This function measures how different is the direction of two vectors, and its\n",
    "values are between -1 and 1. \n",
    "\n",
    "Cosine similarity is defined as:\n",
    "\\begin{equation}\n",
    "    \\operatorname{cos(v, v')} = \\frac{\\sum_{i=1}^{n} v_i  v_i'}{\\sqrt{\\sum_{i=1}^{n} v_i} \\sqrt{\\sum_{i=1}^{n}v_i'}},\n",
    "\\end{equation}\n",
    "where $v$ and $v'$ are vectors with $n$ dimensions.\n",
    "\n",
    "## 8.1 - Question 8 (2 points)\n",
    "\n",
    "**You have to implement two functions:**\n",
    "\n",
    " 1. ```cosine_sim_tf_idf```: computes the cosine similarity of two representations generated using TF-IDF.\n",
    " 2. ```cosine_sim_embedding```: computes the cosine similarity of two field embeddings.\n",
    "\n",
    "\n",
    "*For this question, except for numpy, you cannot use any external python library (e.g., scikit-learn).*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_tf_idf(r1, r2):\n",
    "    \"\"\"\n",
    "    r1: TF-IDF representation. \n",
    "    r2: TF-IDF representation.\n",
    "\n",
    "    :return the cosine similarity of r1 and r2\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_embedding(v1, v2):\n",
    "    \"\"\"\n",
    "    v1: text embedding\n",
    "    v2: text embedding\n",
    "   \n",
    "    :return the cosine similarity of vec1 and vec2\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Feature Extraction\n",
    "\n",
    "We train a logistic regression model to predict whether a pair of reports is duplicate or not. The features employed for the classifion are listed below:\n",
    "\n",
    "1. Cosine similarity of the TF-IDF representation of summary.\n",
    "2. Cosine similarity of the TF-IDF representation of description.\n",
    "2. Cosine similarity of the summary embeddings.\n",
    "2. Cosine similarity of the description embeddings.\n",
    "2. A binary feature that is 1.0 when the report are from the same components. Othewise, it is 0.0.\n",
    "2. A binary feature that is 1.0 when the report are from the same products. Othewise, it is 0.0.\n",
    "\n",
    "\n",
    "## 9.1 - Question 9 (1 point)\n",
    " \n",
    "\n",
    "**Now, implement** ```extract_features```**that extracts the above features from a pair of reports.** \n",
    "\n",
    "*rm_ftr_idxs* is a parameter that allows removing one or more of the original features. If *rm_ftr_idxs=[]*, all features will be used. Otherwise, *rm_ftr_idxs* contains the positions of the features in the previous list to be removed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(r1, r2, rm_ftr_idxs=[]):\n",
    "    \"\"\"\n",
    "    Extract features from a pair (r1, r2).\n",
    "\n",
    "    TF-IDF representation of the summary and descriptions can be acessed on r1 and r2 using the keys 'summary_tfidf' and 'desc_tfidf'.\n",
    "    Summary and description embedding can be acessed on r1 and r2 using the keys 'summary_vec' and 'desc_vec'.\n",
    "\n",
    "\n",
    "    r1: Dictionary that contains all the information about a report\n",
    "    r2: Dictionary that contains all the information about a report\n",
    "    rm_ftr_idxs: Remove features.\n",
    "        For instance, ftr_opt=[1,4] removes cosine similarity of the TF-IDF representation of summary and the cosine similarity of the description embeddings.\n",
    "\n",
    "    :return a vector of real numbers. \n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "#Load labels from the training set\n",
    "y_train = np.asarray([ y for _, _, y in  training_pairs ])\n",
    "\n",
    "def train_clf(rm_ftr_idxs=[]):\n",
    "    # Extract Features\n",
    "    X_train = np.asarray([ extract_features(report_index[r1], report_index[r2], rm_ftr_idxs) for r1, r2, _ in  training_pairs ])\n",
    "    return LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Question 10  (2.5 points)\n",
    "\n",
    "Now, it is time to evaluate the classifier. Besides that, you will have to perform an ablation study to measure the effectiveness of each feature.  Ablation study consist in removing a single component from the original architecture, and measuring how much this isolated modification impacts the model performance. The more a component affects the performance, the more is considered to be effective.\n",
    "\n",
    "For this question, you have to report the classifier accuracy (function ```compute_acc```) for the following configurations:\n",
    "\n",
    "1. Classifier with all features\n",
    "2. Classifier without *cosine similarity of the TF-IDF representation of summary and description*.\n",
    "4. Classifier without *cosine similarity of the summary and description embeddings*.\n",
    "5. Classifier without *component comparison*.\n",
    "5. Classifier without *product comparison*.\n",
    "    \n",
    "*Also, describe the results and findings.*\n",
    "\n",
    "\n",
    "**The mark will be penalized by 1.5 point when the accuracy of the classifier with full features (6 features) is lower than 0.92.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_acc(classifier, X):\n",
    "    # Compute accuracy\n",
    "    y = np.asarray([ y for _, _, y in  validation_pairs ])\n",
    "    return classifier.score(X, y)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
